---
title: Social Hotspot Analysis and Visualization
subtitle: Patterns & Trends in Environmental Data / Computational Movement Analysis Geo 880 --- Semesterproject FS23
author: 
  - name: Aiyana Signer 
    affiliation:
      - name: University of Zurich
  - name: Diego Gomes
    affiliation:
      - name: University of Zurich
date: 07.02.2023
date-format: long
format: html
editor: visual
jupyter: python3
---

## Trying Stuff Out

```{python}
from raster_based_hotspots import RasterBasedPerson, RasterBasedUnionGroup, RasterBasedIntersectGroup
```

```{python}
person = RasterBasedPerson.from_csv(f'data/posmo0.csv', 8)
```

```{python}
person.map_locations()
```

## Abstract

The aim of this project is to identify and characterize social hotspots, locations where people spend time in close proximity at the same time. As such, we investigate and operationalise the spatio-temporal clustering of individuals to uncover patterns of social interaction. A raster-based approach using a hexagonal grid and a point-based approach using a kernel density estimation (KDE) are compared. For the analysis, we use tracking data gathered with the posmo app from a group of students in the Computational Movement Analysis class.

## Introduction and Research Questions

In regards to terminology, a visit refers to a location where an individual spends a certain amount of time. A hotspot is a place with numerous visits, while a social hotspot also takes into account that the visits occur in the same time period.

Analysing social hotspots can provide valuable insights into how people gather, socialize, and engage with their surroundings. Attributes such as average duration, total time spent and the number of visits can be used to measure how popular a gathering spot is. In urban planning, this can be used to help set priorities for public investments in infrastructure (e.g., Kucukpehlivan et al., 2023). In the field of health sciences, hotspot analysis can be used to identify first outbreaks of diseases (Jones et al., 2008) to be used in emergency response planning.

In this project, we compare two different methods to identify hotspots, a raster-based and a point-based approach. Both methods implement an approach to first compute the visits and locations for an individual. This is then further generalized to compute the union and intersection of visits. As such, we try to answer the following research questions:

1.  How can individual hotspot detection from Posmo Tracking data be operationalized and applied to multiple individuals?

2.  How does a cell-based vs. point-based method impact the semantics and certainty of our analysis?

## Methods

<!-- Maybe we can also remove this, since it is partly already described in the introduction!-->

For both the raster-based and the point-based method, we first compute visits of an individual, i.e. locations that a person has spent a certain amount of time. Depending on the modeled environment, location is defined differently. For the raster-based approach, the environment is divided into individual cells, and a a visit is modeled as an individual spending a certain amount of time within a cell. On the other hand, the point-based approach uses a moving centroid to check whether a point is close enough to still be a part of that visit. In both cases, visits that fall below a given threshold time will be considered as a person being in transit, and hence not interpreted as a visit at that location.

The next step is to compute meeting points of multiple individuals, which we operationalised as the union or intersection of visits. For the cell-based approach, near each other means being in the same raster cell. For the point-based approach, a buffer with a certain distance threshold is created around each visit creating a polygon for each visit. The intersection then computes any intersection of the polygons.

### Raster-based

<!-- the following is just a placeholder text, remove it!-->

### Point-based

<!-- the following is just a placeholder text, remove it!-->

## Results

### Raster-based

<!-- the following is just a placeholder text, remove it!-->

### Point-based

<!-- Testing if rendering of code works-->

```{python}
import pandas as pd
import geopandas as gpd
import numpy as np
from scipy.spatial.distance import pdist, squareform
from sklearn.cluster import DBSCAN
from shapely.geometry import Point
from haversine import haversine
import folium
import folium.plugins

class PointBasedEntity:
    def __init__(self):
        self.visits = pd.DataFrame()
        self.locations = gpd.GeoDataFrame()

    def create_locations(self, cluster_radius=0.0025):        
        # Convert locations to 2D numpy array
        points = np.array(self.visits['location'].tolist())
        
        # Create the distance matrix
        dist_matrix = squareform(pdist(points, metric='euclidean'))

        # Perform DBSCAN clustering
        dbscan = DBSCAN(eps=cluster_radius, metric='precomputed', min_samples=1)
        labels = dbscan.fit_predict(dist_matrix)

        # Create a copy of the visits DataFrame
        visits_with_labels = self.visits.copy()
        visits_with_labels['label'] = labels

        # Group the visits by location label
        locations = visits_with_labels.groupby('label')

        # Initialize the locations DataFrame
        self.locations = pd.DataFrame()

        # Compute the statistics for each unique location
        self.locations['num_visits'] = locations.size()
        self.locations['avg_time'] = locations['duration'].mean()
        self.locations['avg_time'] = self.locations['avg_time'].dt.round('S')
        self.locations['total_time'] = locations['duration'].sum()

        # Compute the location centroid for each group
        self.locations['location'] = locations['location'].apply(lambda x: np.mean(np.array(x.tolist()), axis=0))

        # Convert the DataFrame to a GeoDataFrame
        self.locations = gpd.GeoDataFrame(self.locations, geometry=self.locations['location'].apply(lambda x: Point(x[0], x[1])))

    def map_locations(self, column='num_visits'):
        gradient = {
            0.0: '#ffffcc',
            0.1: '#fff1a9',
            0.2: '#fee187',
            0.3: '#feca66',
            0.4: '#feab49',
            0.5: '#fd8c3c',
            0.6: '#fc5b2e',
            0.7: '#ed2e21',
            0.8: '#d41020',
            0.9: '#b00026',
            1.0: '#800026'
        }
        
        # Initialize the map at the center of all locations
        map_center = [47.3666, 8.6795] #[self.locations.geometry.centroid.y.mean(), self.locations.geometry.centroid.x.mean()]
        
        # Create a folium Map
        m = folium.Map(location=map_center, zoom_start=11, tiles='cartodbpositron', max_zoom = 17)

        # Add HeatMap to the map
        data = [[row['geometry'].x, row['geometry'].y, row[column]] for _, row in self.locations.iterrows()]
        folium.plugins.HeatMap(data, gradient=gradient).add_to(m)

        return m

    
class PointBasedPerson(PointBasedEntity):
    def __init__(self, user_id, dist_threshold):
        super().__init__()
        self.user_id = user_id
        self.dist_threshold = dist_threshold  # Distance threshold in km
        self.visits = pd.DataFrame(columns=['start_time', 'end_time', 'location', 'duration'])

    @classmethod
    def from_csv(cls, filename, dist_threshold, visit_threshold=pd.Timedelta('5 minute')):
        data = pd.read_csv(filename)
        
        # Remove rows with missing coordinates
        data = data.dropna(subset=['lat_y', 'lon_x'])

        # Convert the datetime to a pandas datetime object
        data['datetime'] = pd.to_datetime(data['datetime'])

        # Sort the data by datetime
        data = data.sort_values('datetime')

        # Initialize the person
        person = cls(user_id=data.iloc[0]['user_id'], dist_threshold=dist_threshold)

        # Initialize the first visit with the first row
        current_visit = {
            'start_time': data.iloc[0]['datetime'],
            'end_time': data.iloc[0]['datetime'],
            'location': (data.iloc[0]['lat_y'], data.iloc[0]['lon_x']),
            'points': [(data.iloc[0]['lat_y'], data.iloc[0]['lon_x'])]
        }

        # Iterate over the rest of the rows
        for i in range(1, len(data)):
            current_row = data.iloc[i]
            current_location = (current_row['lat_y'], current_row['lon_x'])

            # Calculate centroid of current visit
            lat_centroid = sum(p[0] for p in current_visit['points']) / len(current_visit['points'])
            lon_centroid = sum(p[1] for p in current_visit['points']) / len(current_visit['points'])
            centroid = (lat_centroid, lon_centroid)

            # If the haversine distance to the centroid is above the threshold, it's the end of a visit
            if haversine(centroid, current_location) > person.dist_threshold:
                # Update the end time of the last visit
                current_visit['end_time'] = data.iloc[i - 1]['datetime']

                # Compute the duration of the visit
                current_visit['duration'] = current_visit['end_time'] - current_visit['start_time']
                
                # Update the location of the visit
                current_visit['location'] = centroid

                # Check if the visit duration is long enough to be considered a valid visit
                if current_visit['duration'] >= visit_threshold:
                    # Add the visit to the person
                    person.visits = pd.concat([person.visits, pd.DataFrame([current_visit])], ignore_index=True)

                # Start a new visit
                current_visit = {
                    'start_time': current_row['datetime'],
                    'end_time': current_row['datetime'],
                    'location': current_location,
                    'points': [current_location]
                }
            else:
                # Add the current location to the points of the current visit
                current_visit['points'].append(current_location)

        # Don't forget to check and potentially add the last visit
        current_visit['end_time'] = data.iloc[-1]['datetime']
        current_visit['duration'] = current_visit['end_time'] - current_visit['start_time']
        if current_visit['duration'] >= visit_threshold:
            person.visits = pd.concat([person.visits, pd.DataFrame([current_visit])], ignore_index=True)

        # After all visits are processed, create the locations DataFrame
        person.create_locations()

        return person

    
class PointBasedUnionGroup(PointBasedEntity):
    def __init__(self):
        super().__init__()
        self.people = []
        self.visits = pd.DataFrame()

    def add_person(self, person):
        assert isinstance(person, PointBasedPerson), "Input person is not an instance of the PointBasedPerson class."
        self.people.append(person)

    def compute_visits(self):
        # Combine all visits from all people
        self.visits = pd.concat([person.visits for person in self.people], ignore_index=True)

        # Compute the locations DataFrame
        self.create_locations()

        
class PointBasedIntersectGroup(PointBasedEntity):
    def __init__(self):
        super().__init__()
        self.people = []
        self.visits = pd.DataFrame()

    def add_person(self, person):
        self.people.append(person)

    def compute_visits(self, n_min=2, n=None, min_duration=pd.Timedelta('5 minutes'), cluster_radius=0.0025):
        # Concatenate all visits from all people
        all_visits = pd.concat([person.visits for person in self.people], ignore_index=True)
        
        # Create separate 'lat' and 'lon' columns
        all_visits[['lat', 'lon']] = pd.DataFrame(all_visits['location'].tolist(), index=all_visits.index)

        # Compute the distance matrix
        dist_matrix = squareform(pdist(all_visits[['lat', 'lon']], metric='euclidean'))

        # Apply DBSCAN on the distance matrix
        dbscan = DBSCAN(eps=cluster_radius, metric='precomputed', min_samples=1)
        labels = dbscan.fit_predict(dist_matrix)

        # Assign cluster labels to the visits
        all_visits['cluster'] = labels

        # Compute the centroid for each cluster and assign to each visit within the cluster
        centroid_df = all_visits.groupby('cluster')[['lat', 'lon']].mean().rename(columns={'lat': 'centroid_lat', 'lon': 'centroid_lon'})
        all_visits = all_visits.join(centroid_df, on='cluster')
        all_visits['location'] = list(zip(all_visits['centroid_lat'], all_visits['centroid_lon']))

        # Split each visit into one-minute intervals and assign the cluster label to the segments
        intervals_df = pd.DataFrame()
        for _, visit in all_visits.iterrows():
            time_range = pd.date_range(start=visit['start_time'], end=visit['end_time'], freq='T')
            df = pd.DataFrame({'time': time_range, 'location': [visit['location']]*len(time_range)})
            intervals_df = pd.concat([intervals_df, df], ignore_index=True)

        # Remove seconds from the time values
        intervals_df['time'] = intervals_df['time'].dt.floor('T')

        # Group by cluster and time, and count the number of occurrences
        intervals_count = intervals_df.groupby(['location', 'time']).size().reset_index(name='counts')

        # Filter out the visits where the number of occurrences is less than n or n_min
        if n is None:
            common_intervals = intervals_count[intervals_count['counts'] >= n_min]
        else:
            common_intervals = intervals_count[intervals_count['counts'] == n]

        # Sort by time and group by cluster, and get the start and end times of continuous time blocks
        common_intervals = common_intervals.sort_values('time')
        common_intervals['block'] = (common_intervals['time'].diff() > pd.Timedelta('1 minute')).cumsum()
        common_visits = common_intervals.groupby(['location', 'block']).agg({'time': ['min', 'max'], 'counts': 'size'}).reset_index()
        common_visits.columns = ['location', 'block', 'start_time', 'end_time', 'counts']

        # Convert the counts of times to durations
        common_visits['duration'] = common_visits['counts'] * pd.Timedelta('1 minutes')

        # Filter out the visits where the duration is less than the minimum duration
        common_visits = common_visits[common_visits['duration'] >= min_duration]

        # Drop the unnecessary columns
        common_visits = common_visits.drop(columns=['block', 'counts'])
        
        # Update the visits attribute
        self.visits = common_visits

        # Compute the locations DataFrame
        self.create_locations()
```

```{python}
p = PointBasedPerson.from_csv(f'data/posmo0.csv',  dist_threshold = 0.5)
```

```{python}
ug = PointBasedUnionGroup()
ig = PointBasedIntersectGroup()
```

```{python}
for i in range(3):
    p = PointBasedPerson.from_csv(f'data/posmo{i}.csv', 8)
    ug.add_person(p)
    ig.add_person(p)
    print(f'Finished loading Person {i}')
```

```{python}
ug.compute_visits()
ig.compute_visits()
```

```{python}
ig.map_locations()
```

## Discussion

<!-- the following is just a placeholder text, remove it!-->

## Literature

Jones, K., Patel, N., Levy, M. et al. (2008). Global trends in emerging infectious diseases. Nature 451, 990--993.

Kucukpehlivan, T., Cetin, M., Aksoy, T., Senyel Kurkcuoglu, M. A., Cabuk, S. N., Isik Pekkan, O., Dabanli, A., & Cabuk, A. (2023). Determination of the impacts of urban-planning of the urban land area using GIS hotspot analysis. Computers and Electronics in Agriculture, 210, 107935.

```{Python}
#devtools::install_github("benmarwick/wordcountaddin",  type = "source", dependencies = TRUE)
wordcountaddin:::text_stats()
```
